[{"title":"First Blog Post","type":0,"sectionRef":"#","url":"blog/first-blog-post","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":""},{"title":"Long Blog Post","type":0,"sectionRef":"#","url":"blog/long-blog-post","content":"This is the summary of a very long blog post, Use a &lt;!-- truncate --&gt; comment to limit blog post size in the list view. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":""},{"title":"MDX Blog Post","type":0,"sectionRef":"#","url":"blog/mdx-blog-post","content":"Blog posts support Docusaurus Markdown features, such as MDX. tip Use the power of React to create interactive blog posts. &lt;button onClick={() =&gt; alert('button clicked!')}&gt;Click me!&lt;/button&gt; Copy Click me!","keywords":""},{"title":"Welcome","type":0,"sectionRef":"#","url":"blog/welcome","content":"Docusaurus blogging features are powered by the blog plugin. Simply add Markdown files (or folders) to the blog directory. Regular blog authors can be added to authors.yml. The blog post date can be extracted from filenames, such as: 2019-05-30-welcome.md2019-05-30-welcome/index.md A blog post folder can be convenient to co-locate blog post images: The blog supports tags as well! And if you don't want a blog: just delete this directory, and use blog: false in your Docusaurus config.","keywords":""},{"title":"Basic framework concepts","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/concepts","content":"Basic framework concepts operator - standard airflow operator based on airflow.models.BaseOperator. All operators in mg-airflow are inherited from the base class - mg_airflow.operators.TBaseOperator. work is a temporary repository of intermediate pipeline results. Work in the local file system, sftp file system, s3, postgres are supported. The base class for work is mg_airflow.dag_generator.works.base_work.BaseWork. result is the result of executing the operator (or the task). Result proxies reading and writing to the work. One operator can have only one result. The base class for result is mg_airflow.dag_generator.results.base_result.BaseResult. pass-through/elt is the operator's mode of operation, in which the command is executed on a remote server. At the same time, the result is not uploaded to the airflow-worker. In pass-through operators, reading and writing result is not called. The ETL operator uploads the result from work into the worker's memory, converts it and loads it into postgres, s3 or a file. ELT operators execute code on a remote server. An example could be PgSQL if work is in pg. dag-factory - automatic creation of DAGs from YAML files. Two types of factory are available: when the DAG structure is set completely in YAML and when only the basic properties of the DAG are set in YAML, and operators are set separately by Python code. The code that is responsible for the operation of the factory is mg_airflow.generator.","keywords":""},{"title":"Comparison with other frameworks","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/comparison","content":"","keywords":""},{"title":"DAG configurators for Airflow​","type":1,"pageTitle":"Comparison with other frameworks","url":"docs/data-detective-airflow/comparison#dag-configurators-for-airflow","content":"gusty gusty it works with different input formats, supports TaskGroup.dag-factory dag-factory creates DAGs from YAML. It is also supported by TaskGroup. "},{"title":"DAG orchestrators​","type":1,"pageTitle":"Comparison with other frameworks","url":"docs/data-detective-airflow/comparison#dag-orchestrators","content":"dagster This framework has a lot in common with Apache Airflow. The scheduler and UI are divided into different modules. Work with s3 resources and local files is available. Dagster implements a concept with work, creation and cleaning upon completion of work. There is also a quick scheduler here.Argo Workflows This solution works on Go. Containers are launched in Kubernetes. It is convenient to use because of the isolation of virtual environments. However, there is a difficulty in implementing testing. It is necessary to run pipelines on Go, in which datasets in python will be compared. "},{"title":"dag_generator.dags.python_dag","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/dags/python_dag","content":"","keywords":""},{"title":"PythonDag Objects​","type":1,"pageTitle":"dag_generator.dags.python_dag","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/dags/python_dag#pythondag-objects","content":"class PythonDag(TDag) Copy The class of the dag filled by the TDag.CODE_FILE Arguments: dag_dir: Directory with TDag.META_FILEconfig: Optional and decomposed meta.yaml file "},{"title":"dag_generator.dags.yaml_dag","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/dags/yaml_dag","content":"","keywords":""},{"title":"YamlDag Objects​","type":1,"pageTitle":"dag_generator.dags.yaml_dag","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/dags/yaml_dag#yamldag-objects","content":"class YamlDag(TDag) Copy DAG created based on the description in the yaml file Arguments: dag_dir: Directory with TDag.META_FILEconfig: Optional and decomposed meta.yaml file attach_task​ def attach_task(task: dict) -&gt; None Copy Add task to DAG Arguments: task: Dictionary with task attributes "},{"title":"dag_generator.generator","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/generator","content":"dag_generator.generator dag_generator​ def dag_generator(dag_id_whitelist: Iterable[str] = None) -&gt; Generator[TDag, None, None] Copy DAG Object Generator Arguments: dag_id_whitelist: A list of dag_ids to be generated. If empty, then all will be generated. generate_dag​ def generate_dag(dag_dir: str, dag_id: str = None) -&gt; TDag Copy Get the DAG object by dag_dir @param dag_dir: Full path to the directory @param dag_id: Dag_id is considered the directory name if dag_path is not specified @return:","keywords":""},{"title":"dag_generator.dags.tdag","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/dags/tdag","content":"","keywords":""},{"title":"TDag Objects​","type":1,"pageTitle":"dag_generator.dags.tdag","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/dags/tdag#tdag-objects","content":"class TDag(DAG) Copy The Airflow DAG extension :raises Exception, FileNotFoundError: Arguments: dag_dir: The path to the directory with the dag file (or its YAML)factory: The type of factory to generate, if 'None', then the dag was created without generatingkwargs: Additional arguments get_result​ def get_result(operator: TBaseOperator, result_name: str, result_type: str, work_type: str, work_conn_id: str = None, **kwargs, ,) -&gt; BaseResult Copy Return result. No physical initialization is taking place Raises: ValueError:  Returns: The result of the operator get_work​ def get_work(work_type: str = None, work_conn_id: str = None) -&gt; BaseWork Copy Get work by conn_id. Note! Only the class itself is created here, work is not initialized and is not created Raises: ValueError:  Returns: Work with work_type located in work_conn_id clear_all_works​ def clear_all_works(context: dict) Copy Clearing all works after execution get_all_works​ def get_all_works(context: dict) Copy Clearing all work on completion of execution get_callable_by_def​ def get_callable_by_def(func_def: str) -&gt; Callable Copy Get a function by its description from yaml. Lambdas are supported. If the function is in the project, use it directly. If the function is in code.py - take it from there, otherwise from the global Arguments: func_def: Function description Returns: Callable "},{"title":"dag_generator.results.pg_result","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/results/pg_result","content":"","keywords":""},{"title":"PgResult Objects​","type":1,"pageTitle":"dag_generator.results.pg_result","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/results/pg_result#pgresult-objects","content":"class PgResult(BaseResult) Copy The result stored in the Postgres database get_table_name​ def get_table_name(context) Copy Return the full name of the table with work write_df​ def write_df(obj: DataFrame, context: dict) -&gt; None Copy Write DataFrame to the database Arguments: obj: Dataset for writingcontext: context write​ def write(obj: Any, context: dict) -&gt; None Copy Write DataFrame to the database. Sends a call to write_df Arguments: obj: Dataset for writingcontext: context Raises: TypeError: If the input dataset type is not DataFrame read_df​ def read_df(context: dict) -&gt; DataFrame Copy Read DataFrame from teh database. Sends a call to work.read_df Arguments: context: context Returns: DataFrame read​ def read(context: dict) -&gt; DataFrame Copy Read DataFrame from the database. Sends a call to work.read_df Arguments: context: context Returns: DataFrame "},{"title":"dag_generator.results.base_result","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/results/base_result","content":"","keywords":""},{"title":"ResultType Objects​","type":1,"pageTitle":"dag_generator.results.base_result","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/results/base_result#resulttype-objects","content":"class ResultType(Enum) Copy A class for all types of results. Only these types of results are allowed to be used. Try to avoid writing the result types manually "},{"title":"BaseResult Objects​","type":1,"pageTitle":"dag_generator.results.base_result","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/results/base_result#baseresult-objects","content":"class BaseResult(LoggingMixin, metaclass=ABCMeta) Copy __init__​ def __init__(operator: TBaseOperator, work: BaseWork, name: str, **kwargs) Copy Base class for all results. The result is created at the end of the operator's work and stored into work. The operator always returns one result Arguments: operator: Operator that generates the resultwork: Work in which the result will be savedname: Name of the result with which it will be saved in workkwargs: Additional parameters for BaseResult write_df​ @abstractmethod def write_df(obj: DataFrame, context: dict) Copy This method is responsible for working with the DataFrame Arguments: obj: object to be written to workcontext: context write​ @abstractmethod def write(obj: Union[Any, DataFrame], context: dict) Copy Basic logic of writing the result Arguments: obj: Object to be writtencontext: context read_df​ @abstractmethod def read_df(context: dict) -&gt; DataFrame Copy Read DataFrame Arguments: context: context Returns: Object read from work read​ @abstractmethod def read(context: dict) -&gt; Union[Any, DataFrame] Copy Reading of any result Arguments: context: context Returns: Object read from work "},{"title":"dag_generator.results.pickle_result","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/results/pickle_result","content":"","keywords":""},{"title":"PickleResult Objects​","type":1,"pageTitle":"dag_generator.results.pickle_result","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/results/pickle_result#pickleresult-objects","content":"class PickleResult(BaseResult) Copy Processes the result of a task stored in a bit sequence in a file write_df​ def write_df(obj: Any, context: dict) Copy Write the Data Frame to the file storage. Sends a call to write Arguments: obj: Dataset for writingcontext: context write​ def write(obj: Any, context: dict) Copy Write an object to file storage. Arguments: obj: Object for writingcontext: context read_df​ def read_df(context: dict) Copy Read DataFrame from file storage. Sends a call to work.read Arguments: context: context read​ def read(context: dict) -&gt; Any Copy Read object from file storage. Arguments: context: context Returns: DataFrame "},{"title":"dag_generator.works.base_db_work","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/base_db_work","content":"","keywords":""},{"title":"BaseDBWork Objects​","type":1,"pageTitle":"dag_generator.works.base_db_work","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/base_db_work#basedbwork-objects","content":"class BaseDBWork(BaseWork) Copy Base class for working on a database (Postgres, Greenplum, MySQL) get_xcom_params​ def get_xcom_params(context: dict) -&gt; dict Copy Serialize DBwork into a dictionary for writing to XCom exists​ @abstractmethod def exists(path: str) -&gt; bool Copy Check if the object exists @param path: Path to the object @return: drop​ @abstractmethod def drop(path: str) Copy Delete an object by path @param path: Path to the object @return: set_search_path​ @abstractmethod def set_search_path(search_path: str) Copy Set up the search_path @param search_path: search_path @return: is_schema​ @abstractmethod def is_schema(path: str) -&gt; bool Copy Check if the object is a schema @param path: Path to the object @return: is_table​ @abstractmethod def is_table(path: str) -&gt; bool Copy Check if the object is a table @param path: Path to the object @return: is_view​ @abstractmethod def is_view(path: str) -&gt; bool Copy Check whether an object is a view @param path: Path to the object @return: execute​ @abstractmethod def execute(sql: str) Copy Exeute sql @param sql: SQL query @return: terminate_failed_task_query​ @abstractmethod def terminate_failed_task_query(context: dict) Copy Stop executing the request after receiving the fail status of the task @param context: Task context @return: "},{"title":"dag_generator.works.base_work","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/base_work","content":"","keywords":""},{"title":"WorkType Objects​","type":1,"pageTitle":"dag_generator.works.base_work","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/base_work#worktype-objects","content":"class WorkType(Enum) Copy A class for all types of works. It is allowed to use only these types of works. Writing types of works is highly not recommended.. synchronize​ def synchronize(func: Callable) -&gt; Callable Copy Decorator, which means that the action is to update the state of the object in xcom "},{"title":"BaseWork Objects​","type":1,"pageTitle":"dag_generator.works.base_work","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/base_work#basework-objects","content":"class BaseWork(ABC, LoggingMixin) Copy Base class for processing work work_type: Work type (pickle, pg, gp) Note: each worker will have its own instance of this class created, so one worker will not know that a work has already been created in another worker get_path​ def get_path(context: Dict, prefix: str = 'work_airflow') Copy Return a unique work name for this context (for dag_run) Important note! The name of the work is uniquely determined by dag_run create​ @synchronize def create(context: dict) Copy Create a work. This method does not contain creation logic. The logic of creating specific works in _create_logic clear​ @synchronize def clear(context: dict) Copy Delete a work This method does not contain delete logic. The logic of deleting specific works in _clear_logic get_xcom_key​ def get_xcom_key(context: Dict) Copy Return the key for XCom for the current work and task get_xcom_params​ def get_xcom_params(context: Dict) -&gt; Dict Copy Serialize work to a dictionary for writing to XCom set_params​ def set_params(params: Optional[dict[str, Any]] = None) Copy Установить параметры для класса get_hook​ @abstractmethod def get_hook() Copy Return the hook for work connection_id get_size​ @abstractmethod def get_size(path) -&gt; str Copy Return the size of result in a readable format "},{"title":"dag_generator.works.file_work","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/file_work","content":"","keywords":""},{"title":"FileWork Objects​","type":1,"pageTitle":"dag_generator.works.file_work","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/file_work#filework-objects","content":"class FileWork(BaseFileWork) Copy File system work get_size​ def get_size(path: str) -&gt; str Copy Get the size of the object in the local work. If the object is missing, it returns -1 Arguments: path: Object name Returns: Rounded object size "},{"title":"dag_generator.works.base_file_work","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/base_file_work","content":"","keywords":""},{"title":"BaseFileWork Objects​","type":1,"pageTitle":"dag_generator.works.base_file_work","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/base_file_work#basefilework-objects","content":"class BaseFileWork(BaseWork) Copy Base class for work on file system (sftp, s3 и др.) exists​ @abstractmethod def exists(path: str) -&gt; bool Copy Check if an object located on path exists @param path: The path to the fs object @return: iterdir​ @abstractmethod def iterdir(path: str) -&gt; Generator[str, None, None] Copy Return the path's generator by the specified filepath @param path: The path to the fs object @return: listdir​ @abstractmethod def listdir(path: str) -&gt; List[str] Copy Return the path's list by the provided path @param path: The path to the fs object @return: unlink​ @abstractmethod def unlink(path: str) Copy Delete an object by the provided path @param path: The path to the fs object @return: mkdir​ @abstractmethod def mkdir(path: str) Copy Create a directory by the provided path @param path: The path to the fs object @return: rmdir​ @abstractmethod def rmdir(path: str, recursive: bool = False) Copy Delete a directory by the provided path @param path: The path to the fs object @param recursive: Delete attached files and folders @return: write_bytes​ @abstractmethod def write_bytes(path: str, bts: bytes) Copy Write bytes to a file using the provided path @param path: The path to the fs object @param bts: Content to write @return: read_bytes​ @abstractmethod def read_bytes(path: str) -&gt; bytes Copy Read bytes to a file using the provided path @param path: The path to the fs object @return: is_dir​ @abstractmethod def is_dir(path: str) -&gt; bool Copy Check whether an object is a directory by the provided path @param path: @return: is_file​ @abstractmethod def is_file(path: str) -&gt; bool Copy Check if an object is a file by the provided path @param path: @return: "},{"title":"dag_generator.works.pg_work","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/pg_work","content":"","keywords":""},{"title":"PgWork Objects​","type":1,"pageTitle":"dag_generator.works.pg_work","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/pg_work#pgwork-objects","content":"class PgWork(BaseDBWork) Copy Work based on postgres execute​ @provide_arg('conn', provide_conn) @provide_arg('cur', provide_cur) def execute(sql: str, conn: connection = None, cur: cursor = None, fetch: Optional[str] = None, context: Optional[dict] = None) -&gt; Union[None, tuple, list] Copy Execute SQL script @param sql: SQL-script @param conn: Connection to db @param cur: Cursor @param fetch: None - do not get the result, one - return 1, all - return all @param context: None - Context for syncing a work in XCom @return: get_object_type_by_path​ @provide_arg('conn', provide_conn) @provide_arg('cur', provide_cur) def get_object_type_by_path(path: str, conn: connection = None, cur: cursor = None) -&gt; Union[str, None] Copy The method determines the result type by path. If there is no object DBObjectType.NONE.value will be returned. The input is expected to be path in the form of schema.table_name, so tables and views from the schema located in search_path must be set EXPLICITLY using the schema. Using the DB name in path will also lead to an ERROR is_schema​ @provide_arg('conn', provide_conn) @provide_arg('cur', provide_cur) def is_schema(path: str, conn: connection = None, cur: cursor = None) -&gt; bool Copy Check if the object is a schema @param path: Schema.table @param conn: Connection to db @param cur: Cursor @return: is_table​ @provide_arg('conn', provide_conn) @provide_arg('cur', provide_cur) def is_table(path: str, conn: connection = None, cur: cursor = None) -&gt; bool Copy Check if the object is a table @param path: schema.table @param conn: Connection to db @param cur: Cursor @return: is_view​ @provide_arg('conn', provide_conn) @provide_arg('cur', provide_cur) def is_view(path: str, conn: connection = None, cur: cursor = None) -&gt; bool Copy Check if the object is a view @param path: schema.table @param conn: Connection to db @param cur: Cursor @return: exists​ @provide_arg('conn', provide_conn) @provide_arg('cur', provide_cur) def exists(path: str, conn: connection = None, cur: cursor = None) -&gt; bool Copy Check if the object exists @param path: schema.table @param conn: Connection to db @param cur: Cursor @return: set_search_path​ @provide_arg('conn', provide_conn) @provide_arg('cur', provide_cur) def set_search_path(search_path: str, conn: connection = None, cur: cursor = None) Copy Set search_path @param search_path: search_path @param conn: Connection to db @param cur: Cursor drop​ @provide_arg('conn', provide_conn) @provide_arg('cur', provide_cur) def drop(path: str, conn: connection = None, cur: cursor = None, cascade: bool = True) Copy Drop the object by path @param path: Path to the object @param conn: Connection to db @param cur: Cursor @param cascade: Delete dependent objects write_df​ def write_df(path: str, frame: DataFrame, **kwargs) Copy Upload DataFrame to TEMPORARY TABLE postgres Arguments: The Data Frame index is excluded from loading. Default params: chunksize: 1000 method: 'multi' path: Table nameframe: DataFramekwargs: Additional params get_size​ def get_size(path: str) -&gt; str Copy Get the size of the object in the database. In case of no table returns -1 Arguments: path: Object name Returns: Rounded object size "},{"title":"dag_generator.works.sftp_work","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/sftp_work","content":"","keywords":""},{"title":"SFTPWork Objects​","type":1,"pageTitle":"dag_generator.works.sftp_work","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/sftp_work#sftpwork-objects","content":"class SFTPWork(BaseFileWork) Copy exists​ @provide_sftp def exists(path: str, sftp_client: Optional[SFTPClient] = None) -&gt; bool Copy Check if the path exists on the remote machine Arguments: path: sftp_client:  Returns: execute​ def execute(command: str, sync: bool = False) -&gt; int Copy Run a command on a remote machine Arguments: command: sync:  Returns: mkdir​ def mkdir(path: str, mode: int = 0o777) Copy Recursive directory creation Arguments: path: Path to the directorymode: Rights granted to directories Raises: IOError: OSError:  rmdir​ def rmdir(path: str, recursive: bool = False) Copy Delete by path on a remote machine Arguments: path: Path to the fs objectrecursive: Delete nested objects Raises: IOError: OSError:  write​ @provide_sftp def write(path: str, text: str, sftp_client: SFTPClient = None) Copy Writing text to a file by the path Arguments: path: text: sftp_client:  write_bytes​ @provide_sftp def write_bytes(path: str, bts: bytes, sftp_client: SFTPClient = None) Copy Writing bytes to a file by the path Arguments: path: bts: sftp_client:  read​ @provide_sftp def read(path: str, sftp_client: SFTPClient = None) -&gt; str Copy Reading text by the path Arguments: path: sftp_client:  Returns: read_bytes​ @provide_sftp def read_bytes(path: str, sftp_client: SFTPClient = None) -&gt; bytes Copy Reading bytes from a file by the path Arguments: path: sftp_client:  Returns: is_dir​ @provide_sftp def is_dir(path: str, sftp_client: SFTPClient = None) -&gt; bool Copy Check if the specified path is a directory on the remote machine Arguments: path: sftp_client:  Raises: FileNotFoundError:  Returns: is_file​ @provide_sftp def is_file(path: str, sftp_client: SFTPClient = None) -&gt; bool Copy Check if the specified path is a file on the remote machine Arguments: path: sftp_client:  Returns: Raises: FileNotFoundError:  get_size​ @provide_sftp def get_size(path: str, sftp_client: SFTPClient = None) -&gt; str Copy Get the size of the object in the sftp work. If the object is missing, it returns -1 Arguments: path: Object namesftp_client: Connection Returns: Rounded object size "},{"title":"dag_generator.works.s3_work","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/s3_work","content":"","keywords":""},{"title":"S3Work Objects​","type":1,"pageTitle":"dag_generator.works.s3_work","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/dag_generator/works/s3_work#s3work-objects","content":"class S3Work(BaseFileWork) Copy Work based on s3 get_size​ def get_size(path: str) -&gt; str Copy Get the size of the object in s3. If the object is missing, it returns -1 Arguments: path: Object name Returns: Rounded object size "},{"title":"operators.extractors.python_dump","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/python_dump","content":"","keywords":""},{"title":"PythonDump Objects​","type":1,"pageTitle":"operators.extractors.python_dump","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/python_dump#pythondump-objects","content":"class PythonDump(TBaseOperator) Copy Download data using python code Arguments: python_callable: Python functionop_kwargs: Additional params for python_callablekwargs: Additional params for TBaseOperator "},{"title":"operators.extractors.db_dump","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/db_dump","content":"","keywords":""},{"title":"DBDump Objects​","type":1,"pageTitle":"operators.extractors.db_dump","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/db_dump#dbdump-objects","content":"class DBDump(TBaseOperator) Copy Execute the SQL code in the database and write the result to work Arguments:  or the name of the file with the query in the code folder Copy conn_id: Connection for postgressql: The text of the sql query to be executed,kwargs: Additional params for TBaseOperator "},{"title":"operators.extractors.s3_dump","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/s3_dump","content":"","keywords":""},{"title":"S3Dump Objects​","type":1,"pageTitle":"operators.extractors.s3_dump","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/s3_dump#s3dump-objects","content":"class S3Dump(TBaseOperator) Copy Download files from S3 to DataFrame column named 'response' Arguments:  Connection id Bucket name Path to the object The operator can use the results of other operators to parameterize their queries. The name of the column with the name of the file to download Copy conn_id: Textbucket: Textobject_path: Textsource: Listobject_column: Textkwargs: Additional params for TBaseOperator "},{"title":"operators.extractors.s3_list_bucket","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/s3_list_bucket","content":"","keywords":""},{"title":"S3ListBucket Objects​","type":1,"pageTitle":"operators.extractors.s3_list_bucket","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/s3_list_bucket#s3listbucket-objects","content":"class S3ListBucket(TBaseOperator) Copy Lists keys in a bucket under prefix and not containing delimiter execute returns DataFrame [['key', 'lastmodified', 'etag', 'size', 'storageclass', 'owner']] Arguments:  Connection id Bucket name Limits the response to keys that begin with the specified prefix. A delimiter is a character you use to group keys. Pagination size. Maximum items to return. Copy conn_id: Textbucket: Textprefix: Textdelimiter: Textpage_size: intmax_items: intkwargs: Additional params for TBaseOperator execute​ def execute(context: Optional[dict]) Copy Extended implementation of airflow.hooks.S3_hook.py:155 list_keys. list_keys returns only keys. This implementation returns size and date of last modificationhttps://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.list_objects A DataFrame is written to result:['key', 'lastmodified', 'etag', 'size', 'storageclass', 'owner'] Raises: AirflowBadRequest: With the non-existent bucket Arguments: context: context "},{"title":"operators.extractors.request_dump","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/request_dump","content":"","keywords":""},{"title":"RequestDump Objects​","type":1,"pageTitle":"operators.extractors.request_dump","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/request_dump#requestdump-objects","content":"class RequestDump(TBaseOperator) Copy The RequestDump operator is used for webapi requests Arguments:  Can be a template string. Example: {action}/{subaction}/?format=json Connection id to be used A table that can be used to parameterize queries Example: DataFrame( [ [&amp;#x27;main&amp;#x27;, &amp;#x27;delete&amp;#x27;, ], [&amp;#x27;support&amp;#x27;, &amp;#x27;askForDelete&amp;#x27;, ], ], columns=[&amp;#x27;action&amp;#x27;, &amp;#x27;subaction&amp;#x27;, ] ), The operator can use the results of other operators to parameterize their queries. The result of the source operation will be used in the same way as the DataFrame from url_params. Waiting time between requests in seconds. Copy url: Textconn_id: Texturl_params: DataFramesource: Listwait_seconds: floatkwargs: Additional params for TBaseOperator "},{"title":"operators.sinks.pg_loader","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/pg_loader","content":"","keywords":""},{"title":"PgLoader Objects​","type":1,"pageTitle":"operators.sinks.pg_loader","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/pg_loader#pgloader-objects","content":"class PgLoader(TBaseOperator) Copy Abstract loader for postgres get_table_columns​ @staticmethod def get_table_columns(table_name: str, conn: psycopg2_connection) -&gt; list[str] Copy Get a list of the names of its fields by the name of the table Arguments: table_name: conn:  Returns: Tuple with field names "},{"title":"operators.extractors.tsftpoperator","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/tsftpoperator","content":"","keywords":""},{"title":"TSFTPOperator Objects​","type":1,"pageTitle":"operators.extractors.tsftpoperator","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/extractors/tsftpoperator#tsftpoperator-objects","content":"class TSFTPOperator(TBaseOperator) Copy Get the file remote_filepath from the server with conn_id Arguments: conn_id: Name of the Airflow Connectionremote_filepath: Path to the filekwargs: Additional params for TBaseOperator "},{"title":"operators.sinks.pg_scd1_df_update_insert","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/pg_scd1_df_update_insert","content":"","keywords":""},{"title":"PgSCD1DFUpdateInsert Objects​","type":1,"pageTitle":"operators.sinks.pg_scd1_df_update_insert","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/pg_scd1_df_update_insert#pgscd1dfupdateinsert-objects","content":"class PgSCD1DFUpdateInsert(PgLoader) Copy Update the target table by SCD 1 by diff_change_operation Arguments: source: Sourceconn_id: Connection idtable_name: Table name for updatekey: The key by which update. Avoid NULL for the key.diff_change_oper: Field with the flag of the operation to be applied to the record D,U,Ichunk_row_number: The number of rows in the chunk to load into the database and apply to the table "},{"title":"operators.sinks.pg_scd1","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/pg_scd1","content":"","keywords":""},{"title":"PgSCD1 Objects​","type":1,"pageTitle":"operators.sinks.pg_scd1","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/pg_scd1#pgscd1-objects","content":"class PgSCD1(TBaseOperator) Copy Update the target table by SCD1 Arguments:  loading_method - Download method: Update/Insert (U/I), Delete/Insert(D/I) process_deletions - records that are not in the input temporary will be deleted process_existing_records - records that are not different will not be affected chunk_row_number - the number of rows in the chunk to load into the database and apply to the table only for Update/Insert and DataFrame mode Copy process_existing_records for U/I is enabled by default. source: Sourceconn_id: Connection idtable_name: Table name for updatekey: The key by which update. Avoid NULL for the key.deleted_flg_column: Field with the delete flag, takes the values 0/1kwargs:  upload_and_update_insert​ def upload_and_update_insert(hook: PostgresHook, source_table: str, dataframe: pandas.DataFrame) -&gt; None Copy Load the DataFrame into the database and apply the table to the target. Arguments: hook: Hook for connecting to the databasesource_table: Name for the temporary table in the databasedataframe: DataFrame to apply to a table get_table_columns​ @staticmethod def get_table_columns(table_name: str, conn: psycopg2_connection) -&gt; list[str] Copy Get a list of the names of its fields by the name of the table Arguments: table_name: conn:  Returns: Tuple with field names "},{"title":"operators.sinks.pg_single_target_loader","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/pg_single_target_loader","content":"","keywords":""},{"title":"PgSingleTargetLoader Objects​","type":1,"pageTitle":"operators.sinks.pg_single_target_loader","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/pg_single_target_loader#pgsingletargetloader-objects","content":"class PgSingleTargetLoader(PgLoader) Copy Update the target table in postgres Works only for result_type: pickle By default, it tracks deletions if records were uploaded sometime by the same DAG Arguments: conn_id: Connection idsource: Sourcetable_name: Table name for updatekey: The key by which update. Avoid NULL for the key.filter_callable: Function for filtering strings, optional.deleted_flg: Track deletions, takes the values True/Falsechunk_row_number: Number of lines in chunk to load in partskwargs: "},{"title":"operators.sinks.s3_delete","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/s3_delete","content":"","keywords":""},{"title":"S3Delete Objects​","type":1,"pageTitle":"operators.sinks.s3_delete","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/s3_delete#s3delete-objects","content":"class S3Delete(TBaseOperator) Copy Delete from bucket objects with key filename_columnin S3 with connection conn_idUp to 1000 objects can be deleted in one request.https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.delete_objects Arguments:  Name of the source, put in the list Connection id Bucket name The name of the column containing the path to the file in S3 Number of keys to delete Copy source: Listconn_id: Textbucket: Textfilename_column: Textbatch_size: intkwargs: Additional params for the TBaseOperator execute​ def execute(context: Optional[Dict]) Copy Extended the implementation of airflow.hooks.S3_hook.py:559 Raises: AirflowBadRequest: - at a non-existent bucket "},{"title":"operators.sinks.s3_load","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/s3_load","content":"","keywords":""},{"title":"S3Load Objects​","type":1,"pageTitle":"operators.sinks.s3_load","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/sinks/s3_load#s3load-objects","content":"class S3Load(TBaseOperator) Copy Save data forom bytes_column by the path filename_column with metadata in metadata_columnin S3 with connection conn_id Arguments:  Name of the source, put in the list Connection id Bucket name The name of the column containing the path to the file in S3 The name of the column containing the data. Data type inside: bytes The name of the column containing metadata. The column content should be empty or contain a dictionary with metadata. Copy source: Listconn_id: Textbucket: Textfilename_column: Textbytes_column: Textmetadata_column: Textkwargs: Additional params for the TBaseOperator execute​ def execute(context: Optional[dict]) Copy The download comes from via boto3.s3.inject.upload_fileobj Raises: AirflowBadRequest: - at a non-existent bucket "},{"title":"operators.transformers.append","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/transformers/append","content":"","keywords":""},{"title":"Append Objects​","type":1,"pageTitle":"operators.transformers.append","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/transformers/append#append-objects","content":"class Append(TBaseOperator) Copy Merge multiple objects into one The object must have an append method Arguments: source: Sourcekwargs: Additional params for the TBaseOperator "},{"title":"operators.tbaseoperator","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/tbaseoperator","content":"","keywords":""},{"title":"TBaseOperator Objects​","type":1,"pageTitle":"operators.tbaseoperator","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/tbaseoperator#tbaseoperator-objects","content":"class TBaseOperator(BaseOperator, ABC) Copy Base operator. All other operators need to inherit from this one. Arguments: description: Task descriptionconn_id: Optional connection for connecting to a particular sourcework_conn_id: Optional connection work for the operator (taken from the dag by default)result_type: Optional result type/format (taken from dag by default)work_type: Optional work type/format (taken from dag by default) get_conn_id​ def get_conn_id() -&gt; str Copy Get conn_id from the task or from default DAG settings execute​ @abstractmethod def execute(context) Copy Execute the operator pre_execute​ @prepare_lineage def pre_execute(context: dict = None) Copy The method is called before calling self.execute() post_execute​ @apply_lineage def post_execute(context: dict = None, result=None) Copy The method is called immediately after calling self.execute() generate_context​ def generate_context(execution_date=datetime.now()) -&gt; Dict[str, Any] Copy Generate a context for an execution_date of any kind. read_result​ def read_result(context) Copy Read the result. Used in tests. In statements that do not write to work, you need to redefine. Arguments: context: Execution context Returns: Dataset "},{"title":"operators.transformers.pg_sql","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/transformers/pg_sql","content":"","keywords":""},{"title":"PgSQL Objects​","type":1,"pageTitle":"operators.transformers.pg_sql","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/transformers/pg_sql#pgsql-objects","content":"class PgSQL(TBaseOperator) Copy The class executes sql in the specified Postgres database Creates a table or view based on the passed request Arguments: sql: SQL querysource: Sourceobj_type: Type of the creating object: table, viewanalyze: Analyze (for tables only) (None | * | 'col1, col2, ..., colN')kwargs: Additional params for TBaseOperator execute​ def execute(context: dict) Copy Execute the query to the Postgres Arguments: context: context "},{"title":"operators.transformers.py_transform","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/transformers/py_transform","content":"","keywords":""},{"title":"PyTransform Objects​","type":1,"pageTitle":"operators.transformers.py_transform","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/operators/transformers/py_transform#pytransform-objects","content":"class PyTransform(TBaseOperator) Copy Perform in-memory conversion Arguments: source: List of sourcestransformer: Handler (transformer) functionop_kwargs: Additional params for callablekwargs: Additional params for TBaseOperator execute​ def execute(context: dict) -&gt; None Copy Only in-memory mode is supported Sources are unpacked before entering the function. If there are no sources, the function will not receive them in an empty list. This can be avoided by using the following signature: def transformer(context, *sources): @param context: Execution context "},{"title":"test_utilities.assertions","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/test_utilities/assertions","content":"test_utilities.assertions to_bytes​ def to_bytes(obj: object) -&gt; bytes Copy Convert any object to bytes @param obj: @return: xor_bytes​ def xor_bytes(bts1: bytes, bts2: bytes) -&gt; bytes Copy Make a xor by bytes @param bts1: @param bts2: @return:","keywords":""},{"title":"test_utilities.generate","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/test_utilities/generate","content":"test_utilities.generate is_gen_dataset_mode​ def is_gen_dataset_mode() -&gt; bool Copy Switching the mode of checking and creating test data Add to environment variables export GEN_DATASET=Any # creating unset GEN_DATASET # checking","keywords":""},{"title":"test_utilities.generate_df","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/test_utilities/generate_df","content":"test_utilities.generate_df generate_single_dataframe​ def generate_single_dataframe(columns: Dict, records_count: int = 2, max_str_len: int = 10, max_val_for_numeric_types: int = 100) -&gt; DataFrame Copy A method for generating a dataframe with random data Arguments: columns: Dictionary with columns, key - column, value - type (e.g. id: 'int')records_count: Number of records in the sourcemax_str_len: Maximum length for string datamax_val_for_numeric_types: Maximum value for numeric types Returns: DataFrame filled with generated data generate_dfs_with_random_data​ def generate_dfs_with_random_data(columns: Dict, dataframes_count: int = 10, records_count: int = 2, max_str_len: int = 10, max_val_for_numeric_types: int = 100) -&gt; List Copy Method for generating dataframes with random data Arguments: :param max_val_for_numeric_types: Maximum value for numeric types Copy columns: Dictionary with columns, key - column, value - type (e.g. id: 'int')dataframes_count: Number of sourcesrecords_count: Number of records in the sourcemax_str_len: Maximum length for string data Returns: List of DataFrame with generated data fill_table_from_dataframe​ def fill_table_from_dataframe(conn: psycopg2.extensions.connection, dframe: DataFrame, schema: str, table: str) -&gt; bool Copy A method for filling a table with data from a dataframe. The table must exist. Arguments: conn: Connection to the databasedframe: Dataframe with dataschema: The scheme of the table in the databasetable: Table name in the database Returns: Operation execution status: true-completed, otherwise false.","keywords":""},{"title":"test_utilities.test_helper","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/data_detective_airflow_api_reference/test_utilities/test_helper","content":"test_utilities.test_helper Helper for creating DAG tests run_task​ def run_task(task: Union[TBaseOperator, BaseOperator], context: dict = None) Copy Run a task run_and_read​ def run_and_read(task: Union[TBaseOperator, BaseOperator], context: dict = None) -&gt; DataFrame Copy Run the task and return the DataFrame from the BaseResult instance. run_and_assert_task​ def run_and_assert_task(task: Union[TBaseOperator, BaseOperator], dataset: dict[str, Any], mocker: MockerFixture = None, exclude_cols: list = None, **kwargs) Copy Run the task, get the result and compare Arguments: task: Id of the running taskdataset: Dictionary with comparison examples. Output and input datasets are needed.exclude_cols: Columns excluded from comparisonmocker: MockerFixture fixture run_and_assert​ def run_and_assert(dag: TDag, task_id: str, test_datasets: dict, mocker: MockerFixture, exclude_cols: list = None) Copy Using run_and_assert_task Run the task and if it is TBaseOperator then get the result and compare it with the example Also if the task is PgReplacePartitions then the target table will be cleared first, and then after the launch, compare the contents of the target table with the example Arguments: dag: TDagtask_id: Id of the running tasktest_datasets: Dictionary with examplesexclude_cols: Columns excluded from comparisonmocker: MockerFixture fixture","keywords":""},{"title":"Abstractions for datasets","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/datasets","content":"","keywords":""},{"title":"pandas​","type":1,"pageTitle":"Abstractions for datasets","url":"docs/data-detective-airflow/datasets#pandas","content":"Docs - https://pandas.pydata.org/docs/ The most popular solution for working with tabular data. Allows you to transform data in multiple ways: grouping, filtering, transformations. There are ways to work with a large number of formats: csv, avro, json, markdown and others. The main concepts are Series and DataFrame. When working with a DataFrame, it should be borne in mind that the data in it is stored vertically in columns. As a result, it is unwise to iterate over all columns at once. When working with etl, a common problem is the appearance of NotANumber (NaN, NA, nan). Loading to external sources falls on them. It is usually necessary to clean the NaN before the final uploads. NaN is also usually useful in mathematical calculations. "},{"title":"petl​","type":1,"pageTitle":"Abstractions for datasets","url":"docs/data-detective-airflow/datasets#petl","content":"Docs - https://petl.readthedocs.io/en/stable/ petl works with abstractions over a two-dimensional TupleOfTuples or ListOfList structure. Transformations take place in a functional style. All transformations can be performed in one pass through the dataset. The exceptions are: sorting, grouping, finding unique records. This tool has been developing for a long time and has all the necessary transformations for ETL. It has basic capabilities for working with other formats, so it is preferable to use pandas for conversion tasks to another format. It is considered optimal to work with datasets up to 1 gigabyte. For large volumes, it is better to take a DataFrame. DataFrame stores data more compactly than TupleOfTuples. "},{"title":"Creating a DAG","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/factories","content":"","keywords":""},{"title":"Python Factory​","type":1,"pageTitle":"Creating a DAG","url":"docs/data-detective-airflow/factories#python-factory","content":"This method creates a DAG in semi-automatic mode. The DAG itself is created from the YAML config, empty, without operators. Operators are added to the DAG using python code. To create a dag named TAG_NAME, put the yaml file meta.xml in any subdirectory dags/dags/, for example dags/dags/DAG_NAME or dags/dags/DAG_GROUP/DAG_NAME. Any subdirectory `dogs/dogs/' that has 'meta.yaml' is considered a dag. It has the following parameters: description: DAG descriptiontags - list of tags. It is usually used in filters on the portal and is optionalschedule_interval: schedule_interval airflow param, for example 10 17 * * *default_args: default values owner: airflowretries: 1result_type: type of the result, acceptable values: 'pickle', 'pg'work_type: type of the work, acceptable values: s3, file, pg, sftpwork_conn_id: id of the connection to the work factory: Python (Python factory is being used) Example: description: DAG загрузки метаданных Postgres tags: - postgres schedule_interval: '@once' result_type: Pickle default_args: owner: airflow retries: 1 result_type: pickle work_type: s3 work_conn_id: s3work factory: Python Copy It is also necessary to create dags / dags / DAG_NAME / code / code.py. In this file, the function def fill_dag (dag): must be defined, which adds the necessary statements to the dag. def fill_dag(tdag: TDag): ... DbDump( task_id='meta_get_schemas_main', conn_id=PG, sql='/code/meta_get_schemas.sql', dag=tdag ) ... Copy "},{"title":"YAML Factory​","type":1,"pageTitle":"Creating a DAG","url":"docs/data-detective-airflow/factories#yaml-factory","content":"This method creates a DAG completely automatically from the YAML config. To create a dag named DAG_NAME, put the yaml file meta.yaml in any subdirectorydags/dags/, e.g. dags/dags/DAG_NAME or dags/dags/DAG_GROUP/DAG_NAME. Any subdirectory dags/dags/ that contains meta.yaml is considered a dag. This file should contain the following parameters: description: DAG description tags - tags list. This list is used in filters on the portal schedule_interval: schedule_interval airflow param, for example 10 17 * * *default_args: default values owner: airflowretries: 1result_type: result type, acceptable values: 'pickle', 'pg', 'gp'work_type: type of the work, acceptable values: s3, file, gp, pg, sftpwork_conn_id: id of the connection to the work factory: YAML (YAML factory is being used)tasks: tasks list task_id: task name (unique id)description: task descriptiontype: Task type, class name of one of the helpers library operators, for example PgDump&lt;params&gt;: Parameters required to create a specific task. For example, conn_id, sql It is important that the task parameters in the YAML file contain a complete list of required parameters for the operator constructor. Example: description: Тестовый DAG schedule_interval: '*/5 * * * *' result_type: Pickle default_args: owner: airflow retries: 1 result_type: pickle work_type: s3 work_conn_id: s3work factory: YAML tasks: - task_id: df_now description: Запрос к базе данных type: DbDump conn_id: pg sql: 'select now() as value;' - task_id: append_all description: Объединение предыдущего результата с самим собой type: Append source: - df_now - df_now Copy All additional functions, for example, callable functions for PythonOperator can be specified in the file dags / dags / DAG_NAME / code / code.py. These functions will be automatically loaded when the DAG is generated. Any such function must receive the context as the first parameter. For example def transform(context: dict, df: DataFrame) -&gt; DataFrame: &quot;&quot;&quot;Transform DataFrame :param context: Execution context :param df: Input DataFrame :return: df &quot;&quot;&quot; # etc_dir config = read_config(context['dag'].etc_dir) # Airflow Variable etl_env env = 'dev' if context['var']['value'].etl_env == 'dev' else 'prod' return df Copy This allows access to all dag, task and running properties. "},{"title":"Data Detective Airflow","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/intro","content":"","keywords":""},{"title":"Repository link​","type":1,"pageTitle":"Data Detective Airflow","url":"docs/data-detective-airflow/intro#repository-link","content":"https://github.com/TinkoffCreditSystems/data-detective/tree/master/data-detective-airflow "},{"title":"Running the dev environment using docker-compose","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/development","content":"","keywords":""},{"title":"Setup in PyCharm Professional​","type":1,"pageTitle":"Running the dev environment using docker-compose","url":"docs/data-detective-airflow/development#setup-in-pycharm-professional","content":"Configuring SSH Python interpreter in PyCharm Preferences (CMD + ,) &gt; Project Settings &gt; Project Interpreter Click on the gear icon next to the &quot;Project Interpreter&quot; dropdown &gt; Add Select &quot;SSH Interpreter&quot; &gt; Host: localhost Port: 9922, Username: airflow. Password: see ./Dockerfile:15 Interpreter: /usr/local/bin/python3, Sync folders: Project Root -&gt; /usr/local/airflow Disable &quot;Automatically upload...&quot; Confirm the changes and wait for PyCharm to update the indexes To run tests go to PyCharm -&gt; Edit Configurations -&gt; Environment variables: AIRFLOW_HOME=/usr/local/airflow;PYTHON_PATH=/usr/local/bin/python/:/usr/local/airflow:/usr/local/airflow/dags;AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@metadb:5432/airflow Copy In order not to specify Environment variables every time, they should be added to Templates -&gt; Python tests -&gt; pytest (в Edit Configurations)Setting up py test. To do this, in the Pycharm settings, select Tools-&gt;Python Integrated Tools-&gt;Testing-&gt;Default test runner = pytest. After that, it is possible to run tests by right-clicking on the directory tests or the desired test file. "},{"title":"Working in a dev environment​","type":1,"pageTitle":"Running the dev environment using docker-compose","url":"docs/data-detective-airflow/development#working-in-a-dev-environment","content":"Airflow UI is available at http://localhost:8080/. Login/password to log in - airflow /airflow. To connect to a dev container with airflow, select this options below in PyCharm: Tools &gt; Start SSH Session &gt; Remote Python ... or docker-compose exec app bash (app - service name in docker-compose.yml, bash - command for executing) "},{"title":"Notes​","type":1,"pageTitle":"Running the dev environment using docker-compose","url":"docs/data-detective-airflow/development#notes","content":"At the time of publication, Docker Compose in the command line can be called with a command without a hyphen docker compose .... The documentation will contain examples with docker-compose .... "},{"title":"Production","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/production","content":"Production The start point for generating DAGs is a file in the airflow.settings.DATA_FOLDER with the following contents: import sys import argcomplete from airflow.cli.cli_parser import get_parser from mg_af.constants import DAG_ID_KEY from mg_af.dag_generator import dag_generator dag_id = None if sys.argv[0].endswith('airflow'): parser = get_parser() argcomplete.autocomplete(parser) args = parser.parse_args() dag_id = getattr(args, DAG_ID_KEY, None) whitelist = [dag_id] if dag_id else [] for dag in dag_generator(dag_id_whitelist=whitelist): if not dag: continue globals()['dag_' + dag.dag_id] = dag Copy Next to the file dags/dag_generator.py it is worth placing the file .airflowignore with the contents of dags. This will prevent scanning of py files in the dags folder.dags/dag_generator.py - the only one entry point to the DAGs. Therefore, it does not make sense to parallelize the process of scanning DAGs.At the end of the TDag operation (successful or unsuccessful), the cleanup of all work is called. This process is logged in scheduler. cat dag_generator.py.log | grep callback &gt;2021-05-08 07:23:30,865|INFO|logging_mixin.py:104|&gt;2021-05-08 07:23:30,864|INFO|dag.py:853|Executing dag callback function: &lt;bound method clear_all_works of &lt;DAG: dummy_s3&gt;&gt; Copy The launch of airflow worker should not occur from the root user, for this the airflow user is added to the image.Python modules are installed in ${AIRFLOW_USER_HOME}/.local/binAIR FLOW_HOME can be moved to the /app folder","keywords":""},{"title":"Creating a new operator","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/operators","content":"Creating a new operator Must be inherited from TBaseOperator. class MyTransform(TBaseOperator): def __init__(self, source: List[str], on_columns=None, how='left', **kwargs): super().__init__(**kwargs) self.source = source ... Copy If there is a function parameter in the operator's constructor, then it must be called with the postfix * _callable. For example: class MyTransform(TBaseOperator): def __init__(self, py_callable, **kwargs): ... Copy","keywords":""},{"title":"How to release data-detective-airflow","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/release","content":"How to release data-detective-airflow Create a release with tag like &quot;data-detective-airflow/*&quot; in repo. Action &quot;Publish data-detective-airflow&quot; will be triggered. Package version is extracted from pyproject.toml.","keywords":""},{"title":"Welcome page","type":0,"sectionRef":"#","url":"docs/welcome","content":"Welcome page WIP","keywords":""},{"title":"DAG testing","type":0,"sectionRef":"#","url":"docs/data-detective-airflow/testing","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"DAG testing","url":"docs/data-detective-airflow/testing#notes","content":"DataFrame and other structures in python can contain binary data. Binary data cannot be stored in JSON. In such cases, it is necessary to use decode/encode or abandon JSONPandasDataset.run_and_assert_task uses xor (exclusive or) for comparison and allows not to sort the dataset. "}]