[{"title":"First Blog Post","type":0,"sectionRef":"#","url":"blog/first-blog-post","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":""},{"title":"Long Blog Post","type":0,"sectionRef":"#","url":"blog/long-blog-post","content":"This is the summary of a very long blog post, Use a &lt;!-- truncate --&gt; comment to limit blog post size in the list view. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":""},{"title":"MDX Blog Post","type":0,"sectionRef":"#","url":"blog/mdx-blog-post","content":"Blog posts support Docusaurus Markdown features, such as MDX. tip Use the power of React to create interactive blog posts. &lt;button onClick={() =&gt; alert('button clicked!')}&gt;Click me!&lt;/button&gt; Copy Click me!","keywords":""},{"title":"Welcome","type":0,"sectionRef":"#","url":"blog/welcome","content":"Docusaurus blogging features are powered by the blog plugin. Simply add Markdown files (or folders) to the blog directory. Regular blog authors can be added to authors.yml. The blog post date can be extracted from filenames, such as: 2019-05-30-welcome.md2019-05-30-welcome/index.md A blog post folder can be convenient to co-locate blog post images: The blog supports tags as well! And if you don't want a blog: just delete this directory, and use blog: false in your Docusaurus config.","keywords":""},{"title":"Comparison with other frameworks","type":0,"sectionRef":"#","url":"docs/mg-airflow/comparison","content":"","keywords":""},{"title":"DAG configurators for Airflow​","type":1,"pageTitle":"Comparison with other frameworks","url":"docs/mg-airflow/comparison#dag-configurators-for-airflow","content":"gusty gusty it works with different input formats, supports TaskGroup.dag-factory dag-factory creates DAGs from YAML. It is also supported by TaskGroup. "},{"title":"DAG orchestrators​","type":1,"pageTitle":"Comparison with other frameworks","url":"docs/mg-airflow/comparison#dag-orchestrators","content":"dagster This framework has a lot in common with Apache Airflow. The scheduler and UI are divided into different modules. Work with s3 resources and local files is available. Dagster implements a concept with work, creation and cleaning upon completion of work. There is also a quick scheduler here.Argo Workflows This solution works on Go. Containers are launched in Kubernetes. It is convenient to use because of the isolation of virtual environments. However, there is a difficulty in implementing testing. It is necessary to run pipelines on Go, in which datasets in python will be compared. "},{"title":"Basic framework concepts","type":0,"sectionRef":"#","url":"docs/mg-airflow/concepts","content":"Basic framework concepts operator - standard airflow operator based on airflow.models.BaseOperator. All operators in mg-airflow are inherited from the base class - mg_airflow.operators.TBaseOperator. work is a temporary repository of intermediate pipeline results. Work in the local file system, sftp file system, s3, postgres are supported. The base class for work is mg_airflow.dag_generator.works.base_work.BaseWork. result is the result of executing the operator (or the task). Result proxies reading and writing to the work. One operator can have only one result. The base class for result is mg_airflow.dag_generator.results.base_result.BaseResult. pass-through/elt is the operator's mode of operation, in which the command is executed on a remote server. At the same time, the result is not uploaded to the airflow-worker. In pass-through operators, reading and writing result is not called. The ETL operator uploads the result from work into the worker's memory, converts it and loads it into postgres, s3 or a file. ELT operators execute code on a remote server. An example could be PgSQL if work is in pg. dag-factory - automatic creation of DAGs from YAML files. Two types of factory are available: when the DAG structure is set completely in YAML and when only the basic properties of the DAG are set in YAML, and operators are set separately by Python code. The code that is responsible for the operation of the factory is mg_airflow.generator.","keywords":""},{"title":"Abstractions for datasets","type":0,"sectionRef":"#","url":"docs/mg-airflow/datasets","content":"","keywords":""},{"title":"pandas​","type":1,"pageTitle":"Abstractions for datasets","url":"docs/mg-airflow/datasets#pandas","content":"Docs - https://pandas.pydata.org/docs/ The most popular solution for working with tabular data. Allows you to transform data in multiple ways: grouping, filtering, transformations. There are ways to work with a large number of formats: csv, avro, json, markdown and others. The main concepts are Series and DataFrame. When working with a DataFrame, it should be borne in mind that the data in it is stored vertically in columns. As a result, it is unwise to iterate over all columns at once. When working with etl, a common problem is the appearance of NotANumber (NaN, NA, nan). Loading to external sources falls on them. It is usually necessary to clean the NaN before the final uploads. NaN is also usually useful in mathematical calculations. "},{"title":"petl​","type":1,"pageTitle":"Abstractions for datasets","url":"docs/mg-airflow/datasets#petl","content":"Docs - https://petl.readthedocs.io/en/stable/ petl works with abstractions over a two-dimensional TupleOfTuples or ListOfList structure. Transformations take place in a functional style. All transformations can be performed in one pass through the dataset. The exceptions are: sorting, grouping, finding unique records. This tool has been developing for a long time and has all the necessary transformations for ETL. It has basic capabilities for working with other formats, so it is preferable to use pandas for conversion tasks to another format. It is considered optimal to work with datasets up to 1 gigabyte. For large volumes, it is better to take a DataFrame. DataFrame stores data more compactly than TupleOfTuples. "},{"title":"Running the dev environment using docker-compose","type":0,"sectionRef":"#","url":"docs/mg-airflow/development","content":"","keywords":""},{"title":"Setup in PyCharm Professional​","type":1,"pageTitle":"Running the dev environment using docker-compose","url":"docs/mg-airflow/development#setup-in-pycharm-professional","content":"Configuring SSH Python interpreter in PyCharm Preferences (CMD + ,) &gt; Project Settings &gt; Project Interpreter Click on the gear icon next to the &quot;Project Interpreter&quot; dropdown &gt; Add Select &quot;SSH Interpreter&quot; &gt; Host: localhost Port: 9922, Username: airflow. Password: see ./Dockerfile:15 Interpreter: /usr/local/bin/python3, Sync folders: Project Root -&gt; /usr/local/airflow Disable &quot;Automatically upload...&quot; Confirm the changes and wait for PyCharm to update the indexes To run tests go to PyCharm -&gt; Edit Configurations -&gt; Environment variables: AIRFLOW_HOME=/usr/local/airflow;PYTHON_PATH=/usr/local/bin/python/:/usr/local/airflow:/usr/local/airflow/dags;AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@metadb:5432/airflow Copy In order not to specify Environment variables every time, they should be added to Templates -&gt; Python tests -&gt; pytest (в Edit Configurations)Setting up py test. To do this, in the Pycharm settings, select Tools-&gt;Python Integrated Tools-&gt;Testing-&gt;Default test runner = pytest. After that, it is possible to run tests by right-clicking on the directory tests or the desired test file. "},{"title":"Working in a dev environment​","type":1,"pageTitle":"Running the dev environment using docker-compose","url":"docs/mg-airflow/development#working-in-a-dev-environment","content":"Airflow UI is available at http://localhost:8080/. Login/password to log in - airflow /airflow. To connect to a dev container with airflow, select this options below in PyCharm: Tools &gt; Start SSH Session &gt; Remote Python ... or docker-compose exec app bash (app - service name in docker-compose.yml, bash - command for executing) "},{"title":"Notes​","type":1,"pageTitle":"Running the dev environment using docker-compose","url":"docs/mg-airflow/development#notes","content":"At the time of publication, Docker Compose in the command line can be called with a command without a hyphen docker compose .... The documentation will contain examples with docker-compose .... "},{"title":"MG Airflow","type":0,"sectionRef":"#","url":"docs/mg-airflow/intro","content":"","keywords":""},{"title":"Repository link​","type":1,"pageTitle":"MG Airflow","url":"docs/mg-airflow/intro#repository-link","content":"https://github.com/TinkoffCreditSystems/metadata-governance/tree/master/mg-airflow "},{"title":"Creating a new operator","type":0,"sectionRef":"#","url":"docs/mg-airflow/operators","content":"Creating a new operator Must be inherited from TBaseOperator. class MyTransform(TBaseOperator): def __init__(self, source: List[str], on_columns=None, how='left', **kwargs): super().__init__(**kwargs) self.source = source ... Copy If there is a function parameter in the operator's constructor, then it must be called with the postfix * _callable. For example: class MyTransform(TBaseOperator): def __init__(self, py_callable, **kwargs): ... Copy","keywords":""},{"title":"Creating a DAG","type":0,"sectionRef":"#","url":"docs/mg-airflow/factories","content":"","keywords":""},{"title":"Python Factory​","type":1,"pageTitle":"Creating a DAG","url":"docs/mg-airflow/factories#python-factory","content":"This method creates a DAG in semi-automatic mode. The DAG itself is created from the YAML config, empty, without operators. Operators are added to the DAG using python code. To create a dag named TAG_NAME, put the yaml file meta.xml in any subdirectory dags/dags/, for example dags/dags/DAG_NAME or dags/dags/DAG_GROUP/DAG_NAME. Any subdirectory `dogs/dogs/' that has 'meta.yaml' is considered a dag. It has the following parameters: description: DAG descriptiontags - list of tags. It is usually used in filters on the portal and is optionalschedule_interval: schedule_interval airflow param, for example 10 17 * * *default_args: default values owner: airflowretries: 1result_type: type of the result, acceptable values: 'pickle', 'pg'work_type: type of the work, acceptable values: s3, file, pg, sftpwork_conn_id: id of the connection to the work factory: Python (Python factory is being used) Example: description: DAG загрузки метаданных Postgres tags: - postgres schedule_interval: '@once' result_type: Pickle default_args: owner: airflow retries: 1 result_type: pickle work_type: s3 work_conn_id: s3work factory: Python Copy It is also necessary to create dags / dags / DAG_NAME / code / code.py. In this file, the function def fill_dag (dag): must be defined, which adds the necessary statements to the dag. def fill_dag(tdag: TDag): ... DbDump( task_id='meta_get_schemas_main', conn_id=PG, sql='/code/meta_get_schemas.sql', dag=tdag ) ... Copy "},{"title":"YAML Factory​","type":1,"pageTitle":"Creating a DAG","url":"docs/mg-airflow/factories#yaml-factory","content":"This method creates a DAG completely automatically from the YAML config. To create a dag named DAG_NAME, put the yaml file meta.yaml in any subdirectorydags/dags/, e.g. dags/dags/DAG_NAME or dags/dags/DAG_GROUP/DAG_NAME. Any subdirectory dags/dags/ that contains meta.yaml is considered a dag. This file should contain the following parameters: description: DAG description tags - tags list. This list is used in filters on the portal schedule_interval: schedule_interval airflow param, for example 10 17 * * *default_args: default values owner: airflowretries: 1result_type: result type, acceptable values: 'pickle', 'pg', 'gp'work_type: type of the work, acceptable values: s3, file, gp, pg, sftpwork_conn_id: id of the connection to the work factory: YAML (YAML factory is being used)tasks: tasks list task_id: task name (unique id)description: task descriptiontype: Task type, class name of one of the helpers library operators, for example PgDump&lt;params&gt;: Parameters required to create a specific task. For example, conn_id, sql It is important that the task parameters in the YAML file contain a complete list of required parameters for the operator constructor. Example: description: Тестовый DAG schedule_interval: '*/5 * * * *' result_type: Pickle default_args: owner: airflow retries: 1 result_type: pickle work_type: s3 work_conn_id: s3work factory: YAML tasks: - task_id: df_now description: Запрос к базе данных type: DbDump conn_id: pg sql: 'select now() as value;' - task_id: append_all description: Объединение предыдущего результата с самим собой type: Append source: - df_now - df_now Copy All additional functions, for example, callable functions for PythonOperator can be specified in the file dags / dags / DAG_NAME / code / code.py. These functions will be automatically loaded when the DAG is generated. Any such function must receive the context as the first parameter. For example def transform(context: dict, df: DataFrame) -&gt; DataFrame: &quot;&quot;&quot;Transform DataFrame :param context: Execution context :param df: Input DataFrame :return: df &quot;&quot;&quot; # etc_dir config = read_config(context['dag'].etc_dir) # Airflow Variable etl_env env = 'dev' if context['var']['value'].etl_env == 'dev' else 'prod' return df Copy This allows access to all dag, task and running properties. "},{"title":"Production","type":0,"sectionRef":"#","url":"docs/mg-airflow/production","content":"Production The start point for generating DAGs is a file in the airflow.settings.DATA_FOLDER with the following contents: import sys import argcomplete from airflow.cli.cli_parser import get_parser from mg_af.constants import DAG_ID_KEY from mg_af.dag_generator import dag_generator dag_id = None if sys.argv[0].endswith('airflow'): parser = get_parser() argcomplete.autocomplete(parser) args = parser.parse_args() dag_id = getattr(args, DAG_ID_KEY, None) whitelist = [dag_id] if dag_id else [] for dag in dag_generator(dag_id_whitelist=whitelist): if not dag: continue globals()['dag_' + dag.dag_id] = dag Copy Next to the file dags/dag_generator.py it is worth placing the file .airflowignore with the contents of dags. This will prevent scanning of py files in the dags folder.dags/dag_generator.py - the only one entry point to the DAGs. Therefore, it does not make sense to parallelize the process of scanning DAGs.At the end of the TDag operation (successful or unsuccessful), the cleanup of all work is called. This process is logged in scheduler. cat dag_generator.py.log | grep callback &gt;2021-05-08 07:23:30,865|INFO|logging_mixin.py:104|&gt;2021-05-08 07:23:30,864|INFO|dag.py:853|Executing dag callback function: &lt;bound method clear_all_works of &lt;DAG: dummy_s3&gt;&gt; Copy The launch of airflow worker should not occur from the root user, for this the airflow user is added to the image.Python modules are installed in ${AIRFLOW_USER_HOME}/.local/binAIR FLOW_HOME can be moved to the /app folder","keywords":""},{"title":"How to release mg-airflow","type":0,"sectionRef":"#","url":"docs/mg-airflow/release","content":"How to release mg-airflow Create a release with tag like &quot;mg-airflow/*&quot; in repo. Action &quot;Publish mg-airflow&quot; will be triggered. Package version is extracted from pyproject.toml.","keywords":""},{"title":"DAG testing","type":0,"sectionRef":"#","url":"docs/mg-airflow/testing","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"DAG testing","url":"docs/mg-airflow/testing#notes","content":"DataFrame and other structures in python can contain binary data. Binary data cannot be stored in JSON. In such cases, it is necessary to use decode/encode or abandon JSONPandasDataset.run_and_assert_task uses xor (exclusive or) for comparison and allows not to sort the dataset. "},{"title":"Welcome page","type":0,"sectionRef":"#","url":"docs/welcome","content":"Welcome page WIP","keywords":""}]